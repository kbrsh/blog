<!doctype html><html><head><meta charset='utf-8'><meta http-equiv='X-UA-Compatible' content='IE=edge'><meta name='viewport' content='width=device-width,initial-scale=1'><meta name='description' content="Gradient Descent | Kabir Shah's blog."><meta name='author' content='Kabir Shah'><title>Gradient Descent | Kabir Shah</title><link href='https://fonts.googleapis.com/css?family=Inconsolata|Lora' rel='stylesheet'><link rel='stylesheet' type='text/css' href='../fonts/fonts.css'><link rel='stylesheet' type='text/css' href='../css/lib/wing.min.css'><link rel='stylesheet' type='text/css' href='../css/post.css'><link rel='stylesheet' type='text/css' href='../css/post-math.css'><script>!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject=g,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script",0,"ga"),ga("create","UA-70792533-7","auto"),ga("send","pageview")</script></head><body><div class='container'><a href='../' id='back'><svg width='37' height='24' viewBox='0 0 37 24' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'><title>Arrow</title><path id='back-arrow' d='M36.0607 1.06066c.5857-.585786.5857-1.535534 0-2.12132l-9.546-9.54594c-.5858-.5858-1.5355-.5858-2.1213 0-.5858.5858-.5858 1.53553 0 2.12132L32.8787 0l-8.4853 8.48528c-.5858.58579-.5858 1.53552 0 2.12132.5858.5858 1.5355.5858 2.1213 0l9.546-9.54594zM0 1.5h35v-3H0v3z' transform='rotate(180 18.5 6)' fill='#888'></path></svg></a><h1 class='post-title'>Gradient Descent</h1><h4 class='post-date'>November 1, 2017</h4><div class='post-content'><p>Neural networks work by getting a set of inputs, multiplying them by a set of weights, and then returning an output. At first, the weights are initialized with random values. Eventually, the neural network refines the weights so that it can return a plausible output for a given input.</p><p>A neural network uses a <em>loss function</em> to find how wrong the output was. The goal of the network is to change the weights in such a way that the loss function returns an output close to zero. A low loss value means that the output for all inputs was incredibly close to the expected output for the inputs.</p><p>A method called <em>gradient descent</em> is often used to refine (&quot;train&quot;) the weights so that the loss value gradually becomes smaller.</p><h3 id='two-variables'>Two Variables</h3><p>Instead of jumping right into a 500-dimensional gradient descent example, let&#39;s start out with a simple function that uses two variables, where <span class='mjx-chtml post-math-inline' style='text-align: center'><span class='mjx-math' aria-label='x'><span class='mjx-mrow' aria-hidden='true'><span class='mjx-mi'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.225em; padding-bottom: 0.298em'>x</span></span></span></span></span> is the input, and <span class='mjx-chtml post-math-inline' style='text-align: center'><span class='mjx-math' aria-label='w'><span class='mjx-mrow' aria-hidden='true'><span class='mjx-mi'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.225em; padding-bottom: 0.298em'>w</span></span></span></span></span> is the weight.</p><span class='mjx-chtml MJXc-display post-math' style='text-align: center'><span class='mjx-math' aria-label='f(x, w) = xw
'><span class='mjx-mrow' aria-hidden='true'><span class='mjx-mi'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em'>f</span></span><span class='mjx-mo'><span class='mjx-char MJXc-TeX-main-R' style='padding-top: 0.446em; padding-bottom: 0.593em'>(</span></span><span class='mjx-mi'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.225em; padding-bottom: 0.298em'>x</span></span><span class='mjx-mo'><span class='mjx-char MJXc-TeX-main-R' style='margin-top: -0.144em; padding-bottom: 0.519em'>,</span></span><span class='mjx-mi MJXc-space1'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.225em; padding-bottom: 0.298em'>w</span></span><span class='mjx-mo'><span class='mjx-char MJXc-TeX-main-R' style='padding-top: 0.446em; padding-bottom: 0.593em'>)</span></span><span class='mjx-mo MJXc-space3'><span class='mjx-char MJXc-TeX-main-R' style='padding-top: 0.077em; padding-bottom: 0.298em'>=</span></span><span class='mjx-mi MJXc-space3'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.225em; padding-bottom: 0.298em'>x</span></span><span class='mjx-mi'><span class='mjx-char MJXc-TeX-math-I' style='padding-top: 0.225em; padding-bottom: 0.298em'>w</span></span></span></span></span></div></div><script type='text/javascript' src='../js/scripts.js'></script></body></html>